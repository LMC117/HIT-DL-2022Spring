{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport math\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport os\nimport datetime\nfrom torch.utils.data import DataLoader\nimport argparse\nfrom PIL import Image\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\nfrom timm.models.layers import trunc_normal_, DropPath\nfrom timm.models.registry import register_model\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\n\ntrain_path = os.path.join(\"input\", \"plant-seedlings-classification\", \"train\")\ntest_path = os.path.join(\"input\", \"plant-seedlings-classification\", \"test\")\ntrain_txt_path = os.path.join(\"working\", \"train.txt\")\ndev_txt_path = os.path.join(\"working\", \"dev.txt\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef train_dev_split(img_dir):\n    img_list, label = [], []\n    for root, s_dirs, _ in os.walk(img_dir, topdown=True):\n        for sub_dir in s_dirs:\n            label.append(sub_dir)\n            i_dir = os.path.join(root, sub_dir)\n            list = os.listdir(i_dir)\n            img_list.extend([os.path.join(i_dir, i) for i in list])\n    train, dev = train_test_split(img_list, train_size=0.9, random_state=2)\n    return train, dev, label\n\ndef gen_txt(txt_path, data, label):\n    f = open(txt_path, 'w')\n    for i in data:\n        img_name = i.split('/')[3]\n        line = i + ' ' + str(label.index(img_name)) + '\\n'\n        f.write(line)\n\ntrain, dev, img_label = train_dev_split(train_path)\ngen_txt(train_txt_path, train, img_label)\ngen_txt(dev_txt_path, dev, img_label)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:29.445324Z","iopub.execute_input":"2022-05-28T15:17:29.445717Z","iopub.status.idle":"2022-05-28T15:17:32.238232Z","shell.execute_reply.started":"2022-05-28T15:17:29.445683Z","shell.execute_reply":"2022-05-28T15:17:32.237256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description='caltech')\nparser.add_argument('--gpu', type=str, default='0', help='gpu')\nparser.add_argument('--data_path', type=str, default='../../datasets/caltech-101/101_ObjectCategories',\n                    help='path to train set')\n\nparser.add_argument('--save_dir', type=str, default='working/checkpoint', help='save dir')\nparser.add_argument('--log_dir', type=str, default='working/log', help='log dir')\nparser.add_argument('--save_prefix', type=str, default='working/ResNet18_SGD_e', help='save prefix')\n\nparser.add_argument('--lr_initial', type=float, default=1e-4, help='initial learning rate')\nparser.add_argument('--weight_decay', type=float, default=2e-5, help='weight decay')\n\nparser.add_argument('--batch_size', type=int, default=16, help='batch size')\nparser.add_argument('--epoch_num', type=int, default=60, help='epoch num')\n\nparser.add_argument('--checkpoint_frequency', type=int, default=3, help='checkpoint frequency')\nparser.add_argument('--test_num', type=int, default=2, help='test num')\nopt = parser.parse_args(args=[])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.241289Z","iopub.execute_input":"2022-05-28T15:17:32.242355Z","iopub.status.idle":"2022-05-28T15:17:32.252165Z","shell.execute_reply.started":"2022-05-28T15:17:32.242314Z","shell.execute_reply":"2022-05-28T15:17:32.251195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, txt_path, transform=None, target_transform=None):\n        fh = open(txt_path, 'r')\n        imgs = []\n        for line in fh:\n            line = line.rstrip()\n            idx = line.rfind(' ')\n            imgs.append((line[:idx], int(line[idx:])))\n            self.imgs = imgs\n            self.transform = transform\n            self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        fn, label = self.imgs[index]\n        img = Image.open(fn).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, label\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.253386Z","iopub.execute_input":"2022-05-28T15:17:32.253771Z","iopub.status.idle":"2022-05-28T15:17:32.26742Z","shell.execute_reply.started":"2022-05-28T15:17:32.253736Z","shell.execute_reply":"2022-05-28T15:17:32.266487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vgg_block(nums_conv2d, in_channels, out_channels):\n    blocks = []\n    for _ in range(nums_conv2d):\n        blocks.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1))\n        blocks.append(nn.BatchNorm2d(out_channels, affine=True))\n        blocks.append(nn.ReLU())\n        in_channels = out_channels\n    blocks.append(nn.MaxPool2d(kernel_size=2, stride=2))\n    return nn.Sequential(*blocks)\n\nclass VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        self.conv_arch = ((1, 3, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n        conv_layers = []\n        for (nums_conv2d, in_channels, out_channels) in self.conv_arch:\n            conv_layers.append(vgg_block(nums_conv2d, in_channels, out_channels))\n        self.features = nn.Sequential(*conv_layers)\n        self.F = nn.Flatten()\n        self.Linear = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5))\n        self.Linear2 = nn.Sequential(\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5))\n        self.Linear3 = nn.Linear(4096, 12)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.F(x)\n        x = self.Linear(x)\n        x = self.Linear2(x)\n        x = self.Linear3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.270097Z","iopub.execute_input":"2022-05-28T15:17:32.270624Z","iopub.status.idle":"2022-05-28T15:17:32.284451Z","shell.execute_reply.started":"2022-05-28T15:17:32.270591Z","shell.execute_reply":"2022-05-28T15:17:32.283455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class S_SE(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(S_SE, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Linear(channel, channel // reduction)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(channel // reduction, channel)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: B, C, H, W\n        out = self.avg_pool(x).flatten(1)\n        out = self.sigmoid(self.fc2(self.relu(self.fc1(out))))\n        out = out.unsqueeze(2).unsqueeze(2)\n        return out * x","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.285771Z","iopub.execute_input":"2022-05-28T15:17:32.286145Z","iopub.status.idle":"2022-05-28T15:17:32.297596Z","shell.execute_reply.started":"2022-05-28T15:17:32.286107Z","shell.execute_reply":"2022-05-28T15:17:32.296783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, stride=1, se=False, reduction=4):\n        super(ResBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channel)\n        )\n        if se:\n            self.se_layer = S_SE(channel=out_channel, reduction=reduction)\n        else:\n            self.se_layer = nn.Identity()\n        if in_channel != out_channel:\n            self.short_cut = nn.Sequential(\n                nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=2),\n                nn.BatchNorm2d(out_channel)\n            )\n        else:\n            self.short_cut = nn.Sequential()\n\n    def forward(self, x):\n        # print(self.conv(x).shape)\n        # print(self.short_cut(x).shape)\n        return self.se_layer(self.conv(x)) + self.short_cut(x)\n\n\nclass ResNet18(nn.Module):\n    def __init__(self, class_num, se=False):\n        super(ResNet18, self).__init__()\n        self.residual_layer_nums = [2, 2, 2, 2]\n        self.out_channels = [64, 64, 128, 256, 512]\n        self.se = se\n        self.in_proj = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        self.residual_layer = []\n        self.residual_layer.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n        for i in range(len(self.residual_layer_nums)):\n            layer_num = self.residual_layer_nums[i]\n            in_channel = self.out_channels[i]\n            out_channel = self.out_channels[i + 1]\n            self.residual_layer.append(self.make_layer(ResBlock, layer_num, in_channel, out_channel))\n        self.residual_layer = nn.Sequential(*self.residual_layer)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512, class_num)\n\n    def make_layer(self, block, layer_num, in_channel, out_channel):\n        layer = []\n        for i in range(layer_num):\n            layer.append(block(in_channel, out_channel, stride=2 if in_channel != out_channel and i == 0 else 1,\n                               se=self.se))\n            in_channel = out_channel\n        return nn.Sequential(*layer)\n\n    def forward(self, x):\n        out = self.in_proj(x)  # B, 64, 112, 112\n        out = self.residual_layer(out)  # B, 512, 7, 7\n        out = self.avg_pool(out).flatten(1)  # B, 512\n        out = self.fc(out)  # B, class_num\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.300165Z","iopub.execute_input":"2022-05-28T15:17:32.300871Z","iopub.status.idle":"2022-05-28T15:17:32.318333Z","shell.execute_reply.started":"2022-05-28T15:17:32.300834Z","shell.execute_reply":"2022-05-28T15:17:32.317621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Block(nn.Module):\n    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n    We use (2) as we find it slightly faster in PyTorch\n\n    Args:\n        dim (int): Number of input channels.\n        drop_path (float): Stochastic depth rate. Default: 0.0\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n    \"\"\"\n\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(4 * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)),\n                                  requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        input = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n\n        x = input + self.drop_path(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.319677Z","iopub.execute_input":"2022-05-28T15:17:32.320087Z","iopub.status.idle":"2022-05-28T15:17:32.333282Z","shell.execute_reply.started":"2022-05-28T15:17:32.31998Z","shell.execute_reply":"2022-05-28T15:17:32.332404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvNeXt(nn.Module):\n    r\"\"\" ConvNeXt\n        A PyTorch impl of : `A ConvNet for the 2020s`  -\n          https://arxiv.org/pdf/2201.03545.pdf\n    Args:\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\n        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n    \"\"\"\n\n    def __init__(self, in_chans=3, num_classes=12,\n                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0.,\n                 layer_scale_init_value=1e-6, head_init_scale=1.,\n                 ):\n        super().__init__()\n\n        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n        stem = nn.Sequential(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        cur = 0\n        for i in range(4):\n            stage = nn.Sequential(\n                *[Block(dim=dims[i], drop_path=dp_rates[cur + j],\n                        layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n            )\n            self.stages.append(stage)\n            cur += depths[i]\n\n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n        self.head = nn.Linear(dims[-1], num_classes)\n\n        self.apply(self._init_weights)\n        self.head.weight.data.mul_(head_init_scale)\n        self.head.bias.data.mul_(head_init_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=.02)\n            nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x):\n        for i in range(4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.335675Z","iopub.execute_input":"2022-05-28T15:17:32.336074Z","iopub.status.idle":"2022-05-28T15:17:32.355427Z","shell.execute_reply.started":"2022-05-28T15:17:32.336025Z","shell.execute_reply":"2022-05-28T15:17:32.354628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n    with shape (batch_size, channels, height, width).\n    \"\"\"\n\n    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.eps = eps\n        self.data_format = data_format\n        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n            raise NotImplementedError\n        self.normalized_shape = (normalized_shape,)\n\n    def forward(self, x):\n        if self.data_format == \"channels_last\":\n            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        elif self.data_format == \"channels_first\":\n            u = x.mean(1, keepdim=True)\n            s = (x - u).pow(2).mean(1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.eps)\n            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n            return x","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.356858Z","iopub.execute_input":"2022-05-28T15:17:32.357224Z","iopub.status.idle":"2022-05-28T15:17:32.36929Z","shell.execute_reply.started":"2022-05-28T15:17:32.35719Z","shell.execute_reply":"2022-05-28T15:17:32.368586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class S_MBConv(nn.Module):\n    def __init__(self, channel, reduction):\n        super(S_MBConv, self).__init__()\n        self.bone = nn.Sequential(\n            nn.Conv2d(channel, channel, 1),\n            nn.BatchNorm2d(channel),\n            nn.ReLU(),\n            nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel),\n            SE(channel, reduction),\n            nn.Conv2d(channel, channel, 1),\n            nn.BatchNorm2d(channel)\n        )\n\n    def forward(self, x):\n        return x + self.bone(x)\n\n\nclass My_Improved(nn.Module):\n    def __init__(self, in_channel=3, depths=[3, 3, 9, 3], out_channel=[96, 192, 384, 768], reduction=4, num_classes=12):\n        super(My_Improved, self).__init__()\n        self.downsample_layers = nn.ModuleList()\n        stem = nn.Sequential(\n            nn.Conv2d(in_channel, out_channel[0], kernel_size=4, stride=4),\n            nn.BatchNorm2d(out_channel[0])\n        )\n        self.downsample_layers.append(stem)\n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                nn.BatchNorm2d(out_channel[i]),\n                nn.Conv2d(out_channel[i], out_channel[i + 1], kernel_size=2, stride=2)\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList()\n        for i in range(4):\n            stage = nn.Sequential(\n                *[S_MBConv(out_channel[i], reduction) for _ in range(depths[i])]\n            )\n            self.stages.append(stage)\n\n        self.head = nn.Linear(out_channel[-1], num_classes)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.kaiming_normal_(m.weight, a=math.sqrt(5))\n            nn.init.constant_(m.bias, 0)\n\n    def extract_features(self, x):\n        for i in range(4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n        return x.mean([-2, -1])\n    \n    def forward(self, x):\n        feature = self.extract_features(x)\n        out = self.head(feature)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.371972Z","iopub.execute_input":"2022-05-28T15:17:32.372397Z","iopub.status.idle":"2022-05-28T15:17:32.388712Z","shell.execute_reply.started":"2022-05-28T15:17:32.372361Z","shell.execute_reply":"2022-05-28T15:17:32.387983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_3x3_bn(inp, oup, image_size, downsample=False):\n    stride = 1 if downsample == False else 2\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.GELU()\n    )\n\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn, norm):\n        super().__init__()\n        self.norm = norm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\n\nclass SE(nn.Module):\n    def __init__(self, inp, oup, expansion=0.25):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(oup, int(inp * expansion), bias=False),\n            nn.GELU(),\n            nn.Linear(int(inp * expansion), oup, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass MBConv(nn.Module):\n    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n        super().__init__()\n        self.downsample = downsample\n        stride = 1 if self.downsample == False else 2\n        hidden_dim = int(inp * expansion)\n\n        if self.downsample:\n            self.pool = nn.MaxPool2d(3, 2, 1)\n            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n\n        if expansion == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n                          1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                # down-sample in the first conv\n                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n                          groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.GELU(),\n                SE(inp, hidden_dim),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        \n        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n\n    def forward(self, x):\n        if self.downsample:\n            return self.proj(self.pool(x)) + self.conv(x)\n        else:\n            return x + self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.390067Z","iopub.execute_input":"2022-05-28T15:17:32.390651Z","iopub.status.idle":"2022-05-28T15:17:32.413217Z","shell.execute_reply.started":"2022-05-28T15:17:32.390618Z","shell.execute_reply":"2022-05-28T15:17:32.412176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n        super().__init__()\n        inner_dim = dim_head * heads\n        project_out = not (heads == 1 and dim_head == inp)\n\n        self.ih, self.iw = image_size\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        # parameter table of relative position bias\n        self.relative_bias_table = nn.Parameter(\n            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n\n        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n        coords = torch.flatten(torch.stack(coords), 1)\n        relative_coords = coords[:, :, None] - coords[:, None, :]\n\n        relative_coords[0] += self.ih - 1\n        relative_coords[1] += self.iw - 1\n        relative_coords[0] *= 2 * self.iw - 1\n        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n        self.register_buffer(\"relative_index\", relative_index)\n\n        self.attend = nn.Softmax(dim=-1)\n        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, oup),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: rearrange(\n            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        # Use \"gather\" for more efficiency on GPUs\n        relative_bias = self.relative_bias_table.gather(\n            0, self.relative_index.repeat(1, self.heads))\n        relative_bias = rearrange(\n            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n        dots = dots + relative_bias\n\n        attn = self.attend(dots)\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = self.to_out(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.416471Z","iopub.execute_input":"2022-05-28T15:17:32.416807Z","iopub.status.idle":"2022-05-28T15:17:32.433371Z","shell.execute_reply.started":"2022-05-28T15:17:32.416781Z","shell.execute_reply":"2022-05-28T15:17:32.432589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n        super().__init__()\n        hidden_dim = int(inp * 4)\n\n        self.ih, self.iw = image_size\n        self.downsample = downsample\n\n        if self.downsample:\n            self.pool1 = nn.MaxPool2d(3, 2, 1)\n            self.pool2 = nn.MaxPool2d(3, 2, 1)\n            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n\n        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n        self.ff = FeedForward(oup, hidden_dim, dropout)\n\n        self.attn = nn.Sequential(\n            Rearrange('b c ih iw -> b (ih iw) c'),\n            PreNorm(inp, self.attn, nn.LayerNorm),\n            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n        )\n\n        self.ff = nn.Sequential(\n            Rearrange('b c ih iw -> b (ih iw) c'),\n            PreNorm(oup, self.ff, nn.LayerNorm),\n            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n        )\n\n    def forward(self, x):\n        if self.downsample:\n            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n        else:\n            x = x + self.attn(x)\n        x = x + self.ff(x)\n        return x\n\n\nclass CoAtNet(nn.Module):\n    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n        super().__init__()\n        ih, iw = image_size\n        block = {'C': MBConv, 'T': Transformer}\n\n        self.s0 = self._make_layer(\n            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n        self.s1 = self._make_layer(\n            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n        self.s2 = self._make_layer(\n            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n        self.s3 = self._make_layer(\n            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n        self.s4 = self._make_layer(\n            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n\n        self.pool = nn.AvgPool2d(ih // 32, 1)\n        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n\n    def forward(self, x):\n        x = self.s0(x)\n        x = self.s1(x)\n        x = self.s2(x)\n        x = self.s3(x)\n        x = self.s4(x)\n\n        x = self.pool(x).view(-1, x.shape[1])\n        x = self.fc(x)\n        return x\n\n    def _make_layer(self, block, inp, oup, depth, image_size):\n        layers = nn.ModuleList([])\n        for i in range(depth):\n            if i == 0:\n                layers.append(block(inp, oup, image_size, downsample=True))\n            else:\n                layers.append(block(oup, oup, image_size))\n        return nn.Sequential(*layers)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.434829Z","iopub.execute_input":"2022-05-28T15:17:32.435282Z","iopub.status.idle":"2022-05-28T15:17:32.458619Z","shell.execute_reply.started":"2022-05-28T15:17:32.435247Z","shell.execute_reply":"2022-05-28T15:17:32.457932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_dir = os.path.join(opt.log_dir)\ndatetime_now = datetime.datetime.now().isoformat()[:-7].replace(':', '-')\nlog_txt_name = os.path.join(log_dir, datetime_now + '.txt')\nprint('Now time is : ', datetime_now)\n\nwriter = SummaryWriter(log_dir)\nnum_blocks = [2, 2, 3, 5, 2] \nchannels = [64, 96, 192, 384, 768]\nmodel = ResNet18(class_num=12)\n# from Lab3.model.resnet18 import ResNet18\n# model = ResNet18(class_num=101, se=True)\n# from torchvision.models import resnet18\n# model = resnet18(num_classes=101)\n\nif torch.cuda.device_count() > 1:\n    print('Use', torch.cuda.device_count(), 'GPUs!')\nmodel = torch.nn.DataParallel(model).to(device)\n\n# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr_initial,\n#                        betas=(0.9, 0.999), eps=1e-8, weight_decay=opt.weight_decay)\n\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr_initial,\n                       weight_decay=opt.weight_decay)\n\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.1)\n\nstart_epoch = 1\ncriterion = nn.CrossEntropyLoss().to(device)\n\ntrain_transform = transforms.Compose([\n    # transforms.CenterCrop([224, 224]),\n    transforms.Resize([224, 224]),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\ntest_transform = transforms.Compose([\n    transforms.Resize([224, 224]),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n])\n\ntrain_dataset = MyDataset(txt_path=train_txt_path, transform=train_transform)\ndev_dataset = MyDataset(txt_path=dev_txt_path, transform=test_transform)\ntrain_dataloader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=opt.batch_size)\ndev_dataloader = DataLoader(dataset=dev_dataset, shuffle=False, batch_size=16)\n\ntotal_step = len(train_dataloader)\nprint('Total step: {}'.format(total_step))\n\nepoch_num = opt.epoch_num\nprint('===> Start Epoch {} End Epoch {}'.format(start_epoch, epoch_num))\n\nstep = 0\ndev_acc = 0\n\nfor epoch in range(start_epoch, epoch_num + 1):\n    model.train()\n    epoch_loss = 0.\n    for i, (image, label) in enumerate(tqdm(train_dataloader, desc='Train'), 1):\n        optimizer.zero_grad()\n        image = image.cuda()\n        label = label.cuda().long()\n        pred = model(image)\n        loss = criterion(pred, label.detach())\n        acc = torch.sum(torch.argmax(pred, dim=1) == label) / opt.batch_size\n        epoch_loss += loss.detach().item()\n        batch_log = 'Epoch:[{}/{}] Batch: [{}/{}] loss = {:.4f} lr = {:.7f} acc = {:.3f}'.\\\n            format(epoch, epoch_num, i, total_step, loss.detach().item(), scheduler.get_last_lr()[0], acc)\n        writer.add_scalar('Training_loss', epoch_loss, epoch*total_step+i)\n        writer.add_scalar('Acc in the train dataset', acc, epoch*total_step+i)\n        # print(batch_log)\n        loss.backward()\n        optimizer.step()\n        \n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in dev_dataloader:\n            images, labels = images.to(device), labels.to(device)\n            output = model(images)\n            _, predicted = torch.max(output.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        print('Accuracy of the network on the dev images: {} %'.format(100 * correct / total))\n        writer.add_scalar('Acc in the dev dataset', 100 * correct / total, epoch)\n\n        if (100 * correct / total) > dev_acc:  # 寻找最高准确率的模型参数\n            torch.save(model.state_dict(), opt.save_prefix + str(epoch) + '.pth')\n            dev_acc = 100 * correct / total","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:17:32.459816Z","iopub.execute_input":"2022-05-28T15:17:32.460621Z","iopub.status.idle":"2022-05-28T16:25:43.268036Z","shell.execute_reply.started":"2022-05-28T15:17:32.460585Z","shell.execute_reply":"2022-05-28T16:25:43.267178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nclass TestDataset(Dataset):\n    def __init__(self, test_dir, transform=None):\n        img_list = os.listdir(test_dir)\n        img_list = [os.path.join(test_dir, img_list[i]) for i in range(len(img_list))]\n        self.imgs = img_list\n        self.transform = transform\n\n    def __getitem__(self, index):\n        fn = self.imgs[index]\n        img = Image.open(fn).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return len(self.imgs)\n    \ndef predict_class():\n    pth = os.path.join('working', 'ResNet18_SGD_e57.pth')\n    test_dataset = TestDataset(test_dir=test_path,transform=test_transform)\n    test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)\n    model = ResNet18(class_num=12)\n    model = torch.nn.DataParallel(model).to(device)\n    model.load_state_dict(torch.load(pth))\n    model.eval()\n    with torch.no_grad():\n        predict_list = []\n        for img in test_dataloader:\n            img = img.to(device)\n            predict = model(img)\n            max_index = torch.argmax(predict.data, dim=1)\n            predict_list.extend(max_index.cpu().data.numpy().tolist())\n    \n    file_predict_table = [[os.listdir(test_path)[i], img_label[predict_list[i]]] for i in range(len(predict_list))]\n    df = pd.DataFrame(file_predict_table, columns=['file','species'])\n    df.to_csv(\"working/ResNet18_SGD.csv\", index=False)\n\npredict_class()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T16:27:03.759339Z","iopub.execute_input":"2022-05-28T16:27:03.759726Z","iopub.status.idle":"2022-05-28T16:27:12.234417Z","shell.execute_reply.started":"2022-05-28T16:27:03.759693Z","shell.execute_reply":"2022-05-28T16:27:12.2336Z"},"trusted":true},"execution_count":null,"outputs":[]}]}